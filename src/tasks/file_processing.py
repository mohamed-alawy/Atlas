from celery_app import celery_app, get_setup_utils
from helpers.config import get_settings
import asyncio
from models.ProjectModel import ProjectModel
from models.ChunkModel import ChunkModel
from models.AssetModel import AssetModel
from models.db_schemas import DataChunk
from models import ResponseStatus
from models.enums.AssetTypeEnum import AssetTypeEnum
from controllers import ProcessController
from controllers import NLPController
import logging

logger = logging.getLogger(__name__)

@celery_app.task(bind=True, name="tasks.file_processing.process_project_files")
def process_project_files(self, project_id: int, 
                          file_id: int, chunk_size: int,
                          overlap_size: int, do_reset: int):

    # Return the result of the coroutine so Celery task gets the payload
    asyncio.run(
        _process_project_files(self, project_id, file_id, chunk_size,
                               overlap_size, do_reset)
    )


async def _process_project_files(task_instance, project_id: int, 
                                 file_id: int, chunk_size: int,
                                 overlap_size: int, do_reset: int):

    db_engine, vector_db_client = None, None
    
    try:

        (db_engine, db_client, llm_provider_factory, 
        vectordb_provider_factory,
        generation_client, embedding_client,
        vector_db_client, template_parser) = await get_setup_utils()

        project_model = await ProjectModel.create_instance(db_client)
        project = await project_model.get_project_or_create_one(project_id)

        asset_model = await AssetModel.create_instance(db_client)

        nlp_controller = NLPController(
            vector_db_client=vector_db_client,
            embedding_client=embedding_client,
            generation_client=generation_client,
            template_parser=template_parser,
        )

        project_files_ids = {}

        if file_id:
            asset_record = await asset_model.get_asset_record_by_id(project.project_id, int(file_id))        
            if not asset_record:
                task_instance.update_state(
                    state="FAILURE",
                    meta={"signal": ResponseStatus.FILE_NOT_FOUND.value}
                )

                raise Exception(f"No assets for file: {file_id}")

            project_files_ids = {
                asset_record.asset_id:asset_record.asset_name
            }
        else:
            project_assets = await asset_model.get_all_assets(project.project_id, AssetTypeEnum.FILE.value)
            project_files_ids = {
                record.asset_id:record.asset_name
                for record in project_assets
            }

        if len(project_files_ids) == 0:
            task_instance.update_state(
                state="FAILURE",
                meta={"error": ResponseStatus.NO_FILES_TO_PROCESS.value}
            )

            raise Exception(f"No files found for project_id: {project.project_id}")
        
        process_controller = ProcessController(project_id)

        no_records = 0
        no_files = 0

        chunk_model = await ChunkModel.create_instance(db_client)

        if do_reset:
            # Delete collection from VectorDB
            collection_name = nlp_controller.create_collection_name(project_id=project.project_id)
            _ = await vector_db_client.delete_collection(collection_name)

            # Delete chunks from DB
            _ = await chunk_model.delete_chunks_by_project_id(project.project_id)

        for asset_id, file_id in project_files_ids.items():
            file_content = process_controller.get_file_content(file_id)

            if file_content is None:
                logger.error(f"Error while processing file: {file_id}")
                continue

            file_chunks = process_controller.process_file_content(
                file_content=file_content,
                chunk_size=chunk_size,
                chunk_overlap=overlap_size
            )

            if file_chunks is None or len(file_chunks) == 0:
                logger.error(f"No chunks for file_id: {file_id}")
                pass

            file_chunks_records = [
                DataChunk(
                    chunk_text=chunk.page_content,
                    chunk_metadata=chunk.metadata,
                    chunk_order=i+1,
                    chunk_project_id=project.project_id,
                    chunk_asset_id=asset_id
                )
                for i, chunk in enumerate(file_chunks)
            ]

            no_records += await chunk_model.insert_multiple_chunks(chunks=file_chunks_records)
            no_files += 1

        task_instance.update_state(
            state="SUCCESS",
            meta={"message": ResponseStatus.FILE_PROCESSING_SUCCESS.value}
        )

        logger.warning(f"inserted_chunks: {no_records}")

        return {
                    "message": ResponseStatus.FILE_PROCESSING_SUCCESS.value,
                    "inserted_chunks": no_records,
                    "processed_files": no_files
                }
    
    except Exception as e:
        logger.error(f"Task failed: {str(e)}")
        raise
    finally:
        try:
            if db_engine:
                await db_engine.dispose()
            
            if vector_db_client:
                await vector_db_client.disconnect()
        except Exception as e:
            logger.error(f"Task failed while cleaning: {str(e)}")